{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soiaporn model\n",
    "\n",
    "Prototype the Metropolis-within-Gibbs sampling technique presented in Soiaporn et al. 2012.\n",
    "\n",
    "The full conditionals:\n",
    "\n",
    "$$\n",
    "F_T | f, \\lambda, D \\sim Gamma \\bigg(N_C +1, \\frac{1}{1/s + (1 - f)\\epsilon_0 + f \\sum_{k \\geq 1} w_k \\epsilon_k}\\bigg)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\lambda_i | F_T, f, D) \\propto \\frac{f_{\\lambda_i}}{\\epsilon_{\\lambda_i}} h_{\\lambda_i}\n",
    "$$\n",
    "\n",
    "Where $h_{j} = (1 - f) \\epsilon_0$ if $j = 0$ and $h_j = fw_j\\epsilon_j$ if $j \\geq 1$.\n",
    "\n",
    "$$\n",
    "P(f | \\lambda, F_T, D) \\propto e^{-F_T [  (1 - f)\\epsilon_0  + f \\sum_{k \\geq 1} \\epsilon_k w_k] } \\times (1-f)^{m_0(\\lambda) + b - 1}f^{N_C - m_0(\\lambda)+a-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$F_T$ and $\\lambda$ are sampled directly from the gamma and multinomial distributions. $f$ is sampled using a random walk Metropolis algorithm with Gaussian proposals centred on the current value of $f$. The variance of the proposal distribution was tuned to give an acceptance rate of 25%.\n",
    "\n",
    "$\\kappa$ is treated specially, they consider a logarithmically spaced grid of values to condition on. So, treat $\\kappa$ as fixed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T11:54:15.799744Z",
     "start_time": "2018-06-29T11:54:15.796676Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-29T12:38:05.201459Z",
     "start_time": "2018-06-29T12:38:05.142137Z"
    }
   },
   "outputs": [],
   "source": [
    "def scale_F_T(s, f, eps, w):\n",
    "    sum_term = 0\n",
    "    for k in range(len(eps)) + 1:\n",
    "        sum_term += w[k] * eps[k] \n",
    "    denom = (1 + s) + ((1 - f) * eps[0]) + (f * sum_term)\n",
    "    return 1/ denom\n",
    "\n",
    "def prob_lam_i(k, F_T, f, kappa, kappa_c, eps, w, varpi, d):\n",
    "    if i == 0:\n",
    "        h = (1 - f) * eps[0]\n",
    "    else:\n",
    "        h = f * w[k] * eps[k]\n",
    "    term1 = fik(kappa, kappa_c, d, varpi[i]) / eps[i]\n",
    "    return term1 * h\n",
    "\n",
    "def fik(kappa, kappa_c, d, varpi, A, theta_i):\n",
    "    term1 = kappa * kappa_c / (4 * sinh(kappa) * sinh(kappa_c))\n",
    "    inner = np.linalg.norm((kappa_c * d) + (kappa * varpi))\n",
    "    term2 = sinh(inner) / inner\n",
    "    return A * np.cos(theta) * term1 * term2\n",
    "\n",
    "def p_f(F_T, f, eps, lam, N_C, a, b):\n",
    "    sum_term = 0\n",
    "    for k in range(len(eps)) + 1:\n",
    "        sum_term += w[k] * eps[k]\n",
    "    inner = -F_T * ((1 - f) * eps[0] + f * sum_term)\n",
    "    term1 = np.exp(inner)\n",
    "    m_0 = (lam == 0).sum()\n",
    "    term2 = (1 - f)**(m_0 + b - 1) * f**(N_C - m_0 + a - 1)\n",
    "    return term1 * term2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose initial values\n",
    "kappa = 100\n",
    "kappa_c = 1000\n",
    "F_T = 0.2 \n",
    "f = 0.5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bayes)",
   "language": "python",
   "name": "bayes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
